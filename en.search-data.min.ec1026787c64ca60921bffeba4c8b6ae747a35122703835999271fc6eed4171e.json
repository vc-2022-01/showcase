[{"id":0,"href":"/showcase/docs/Members/","title":"Members","section":"Docs","content":" Miembros # En esta sección se presentarán los integrantes para la presentación de los trabajos de Computación Visual. Conformados por:\nAndrés Felipe Betancurth Becerra Juan Manuel Correa Lombana Gabriela María García Romero "},{"id":1,"href":"/showcase/docs/Members/Andr%C3%A9s-Felipe-Betancurth-Becerra/","title":"Andrés Felipe Betancurth Becerra","section":"Members","content":" Andrés Felipe Betancurth Becerra # estudiante de decima matricula de Ingeniería de Sistemas y Computación en la Universidad Nacional de Colombia con lo que es practicamente solo la mitad de avance de mi carrera, me interesa el campo de la inteligencia artificial orientado al maching lerning, las curiosidades y las series.\n"},{"id":2,"href":"/showcase/docs/Members/Gabriela-Mar%C3%ADa-Garc%C3%ADa-Romero/","title":"Gabriela María García Romero","section":"Members","content":" Gabriela María García Romero # Estudiante de octavo semestre de Ingeniería de Sistemas y Computación en la Universidad Nacional de Colombia. Con interés en temas de modelización e inteligencia artificial. Actualmente se desempeña como representante estudiantil ante el Comité Asesor de la carrea.\nHace parte de los siguientes grupos:\nSemillero Laboratorio de Investigación en Sistemas Inteligentes (LISI) WIE (Women in Engineering) "},{"id":3,"href":"/showcase/docs/Members/Juan-Manuel-Correa-Lombana/","title":"Juan Manuel Correa Lombana","section":"Members","content":" Juan Manuel Correa Lombana # Estudiante de octavo semestre de Ingeniería de Sistemas y Computación en la Universidad Nacional de Colombia. , con 1 año de experiencia en manejo de datos y ciencia de datos. Entre sus proyectos realizados se encuentra:\n6 meses en un proyecto de grafos y optimización de rutas de transporte usando inteligencia artificial 8 meses como analista de datos y ciencia de datos Actualmente llevo 2 meses en ingeniería de datos en el área de arquitectura de bases de datos "},{"id":4,"href":"/showcase/docs/Perlin-Noise/","title":"Perlin Noise","section":"Docs","content":" Ruido de Perlin # El ruido de perlin es una función utilizada en la computación gráfica para crear texturas primitivas. Este fue desarrollado por Ken Perlin en 1983, cuando publicó un artículo llamado An Image Synthesizer.\n¿Qué es ruido? # El ruido es una función pseudo-aleatoria a partir de la cual se pueden generar texturas. Esta se puede representar como una rejilla como la mostrada a continuación:\nEn donde cada vertice tiene asociado un vector\nun vector es un segmento de recta con magnitud y dirección Este es un vector gradiente pseudo aleatorio.\nAsí, el valor del ruido Perlin en el punto se calcula como un producto punto entre el punto entre los vectores de gradiente en los vértices de la grilla y los vectores desde el punto dado a estos vértices.\nPara finalizar, de interpola el resultado con una función. Generalmente se utiliza este polinomio cúbico:\n3x^2 -2x^3 Características # Entre sus características están:\nPseudo-aleatoria Invariante estadísticamente bajo la rotación y translación Tiene un filtro pasa bandas en su frecuencia Esto permite crear superficies a diferentes escalas, y sin perder el control del efecto al rotar y trasladar.\nNaturalidad # El objetivo de Perlin con el diseño del algoritmo era la generación de gráficos que fueran más naturales, es decir, que emulen movimientos y texturas de la naruraleza, obteniendo texturas \u0026ldquo;realistas\u0026rdquo;. Esto lo hace al crear secuencias naturalmente ordenadas y suaves de números pseudoaleatorios.\nEs por esta razón que el ruido de Perlin se ha utilizado para crear representaciones convincentes de nubes, fuego, agua, estrellas, tierra, entre otros.\nReferencias # Michot-Roberto, S., Garcia-Hernández, A., Dopazo-Hilario, S., \u0026amp; Dawson, A. (2021). The spherical primitive and perlin noise method to recreate realistic aggregate shapes. Granular Matter, 23(2), 1-11.\nPerlin, K. (1985). An image synthesizer. ACM Siggraph Computer Graphics, 19(3), 287-296.\nTatarinov, A. (2008). Perlin noise in real-time computer graphics. In GraphiCon (pp. 177-183).\nEn.wikipedia.org. 2022. Perlin noise - Wikipedia. [online] Available at: https://en.wikipedia.org/wiki/Perlin_noise [Accessed 5 April 2022].\n"},{"id":5,"href":"/showcase/docs/Perlin-Noise/Particule/","title":"Particule","section":"Perlin Noise","content":" Movimiento de partículas a través de un campo de perlin noise # En el siguiente frame se puede ver el movimiento con rastro de partículas a través de un campo de perlin noise\nsi jugamos con la rejilla y la dirección de los vectores, se pueden generar texturas o efectos visuales más complejos\nSnow # Grass # Según lo explica Andre Tatarinov en su paper titulado Perlin noise in Real-time Computer Graphics, esto también se puede usar para generar efectos dinámicos volumétricos como fuego, una explosión o humo, esto solo cambiando la dirección y el comportamiento de los vectores asociados a la rejilla.\n"},{"id":6,"href":"/showcase/docs/Perlin-Noise/Terrain/","title":"Terrain","section":"Perlin Noise","content":" Terreno procedural # {{\u0026lt; let terrain = []; function setup() { createCanvas(500, 500, WEBGL); cols = 0;rows = 0; elevacion = 100; scl = 25; vel = 0.05; cambio = scl; crecimiento = 0.07; w = 800; h = 800; cols = w/scl; rows = h/scl; avance = 0; } function stripOfTerrain(){ yoff = avance; xoff = 0; for(x = 0; x \u0026lt; (cols);x++){ terrain[x] = []; } for(y = 0; y \u0026lt; (rows); y++){ xoff = 0; for(x = 0; x \u0026lt; (cols);x++){ terrain[x][y] = map(noise(xoff,yoff),0,1,-elevacion,elevacion); xoff += crecimiento; } yoff += crecimiento; } } function draw() { stripOfTerrain(); cols = w/scl; rows = h/scl; avance -= vel; background(\u0026#39;blue\u0026#39;); stroke(255); rotateX(PI/3); translate(-375,-525); for(y = 0; y \u0026lt; rows; y++){ beginShape(TRIANGLE_STRIP); for(x = 0; x \u0026lt; cols;x++){ fill(10,200,200-terrain[x][y]*10) vertex(x*scl,y*scl,terrain[x][y]); fill(10,255,100-terrain[x][y+1]*10) vertex(x*scl,(y+1)*scl,terrain[x][y+1]); } endShape(); } } \u0026gt;}} All parameters are optional but sketch. Default values are shown in the above snippet but for libs*. Up to lib5 libs may be specified.\n"},{"id":7,"href":"/showcase/docs/Rendering/1.-Baseline/","title":"1. Baseline","section":"Rendering","content":" Proportionality and symmetry # The key to the illusion realized in the game by forced perception is the proportionality that exists between the field of view and the object. The object grows as much as the field of view. This can be demonstrated by the proportionality theorem for triangles, where the ratio between z\u0026rsquo;/z is equivalent to the ratio between x\u0026rsquo;/x, since they have the same θ the angle of the field of view:\n\\[ \\frac{x\u0026#39;}{x} = \\frac{z\u0026#39;}{z} \\] entonces\n\\[ x\u0026#39; = \\frac{z\u0026#39;*x}{z} \\] The same applies to the y-axis:\n\\[ \\frac{y\u0026#39;}{y} = \\frac{z\u0026#39;}{z} \\] therefore\n\\[ y\u0026#39; = \\frac{z\u0026#39;*y}{z} \\] That is, a change in the distance at which the object is zoomed out will be proportional to the size at which the object grows. And the perception obtained by the camera will be symmetrical.\nCode base # Below is the code in which the phenomenon of forced perspective is explored, this has two scenes, one is the camera and the other is the screen space. Through the following code you can see how the changes made from the camera perspective are reflected in the screen space.\nInstructions\nSelect one of the boxes by pressing one of the numbers from 0 to 9. The box selected will turn red. Use the letter w move the box further from the camera and the letter z to move it closer. Use the slider to adjust the zoom of the camera. On the left you can see how the box remains the same size, while on the right you can see how the object is increasing or decreasing in size in the world. Base superliminal let fbo1, fbo2; let cam1, cam2; let target = 150; let length = 600; let boxes; let box_key; let fovy; const SPEED = 5; function setup() { createCanvas(length, length / 2); // frame buffer object instances (FBOs) fbo1 = createGraphics(width / 2, height, WEBGL); fbo2 = createGraphics(width / 2, height, WEBGL); // FBOs cams cam1 = new Dw.EasyCam(fbo1._renderer, { distance: 200 }); let state1 = cam1.getState(); cam1.attachMouseListeners(this._renderer); cam1.state_reset = state1; // state to use on reset (double-click/tap) cam1.setViewport([0, 0, width / 2, height]); cam2 = new Dw.EasyCam(fbo2._renderer, { rotation: [0.94, 0.33, 0, 0] }); cam2.attachMouseListeners(this._renderer); let state2 = cam2.getState(); cam2.state_reset = state2; // state to use on reset (double-click/tap) cam2.setViewport([width / 2, 0, width / 2, height]); document.oncontextmenu = function () { return false; } // scene colorMode(RGB, 1); let trange = 100; boxes = []; for (let i = 0; i \u0026lt; 10; i++) { boxes.push( { position: createVector((random() * 2 - 1) * trange, (random() * 2 - 1) * trange, (random() * 2 - 1) * trange), size: random() * 25 + 8, color: color(random(), random(), random()) } ); } fovy = createSlider(PI / 12, PI * (11 / 12), PI / 3, PI / 48); fovy.position(10, 10); fovy.style(\u0026#39;width\u0026#39;, \u0026#39;80px\u0026#39;); } function draw() { fbo1.background(200, 125, 115); fbo1.reset(); fbo1.perspective(fovy.value()); fbo1.axes(); fbo1.grid(); scene1(); beginHUD(); image(fbo1, 0, 0); endHUD(); fbo2.background(130); fbo2.reset(); fbo2.axes(); fbo2.grid(); scene2(); fbo2.viewFrustum(fbo1); beginHUD(); image(fbo2, width / 2, 0); endHUD(); } function scene1() { boxes.forEach(box =\u0026gt; { fbo1.push(); fbo1.fill(boxes[box_key] === box ? color(\u0026#39;red\u0026#39;) : box.color); fbo1.translate(box.position); if (boxes[box_key] === box) { if (keyIsPressed \u0026amp;\u0026amp; !mouseIsPressed) { let boxLocation = fbo1.treeLocation([0, 0, 0], { from: fbo1.mMatrix(), to: \u0026#39;WORLD\u0026#39; }); let pixelRatio = fbo1.pixelRatio(boxLocation); box.target ??= box.size / pixelRatio; box.size = box.target * pixelRatio; let eyeLocation = fbo1.treeLocation([0, 0, 0], { from: \u0026#39;EYE\u0026#39;, to: \u0026#39;WORLD\u0026#39; }); box.position.add(p5.Vector.sub(boxLocation, eyeLocation).normalize().mult(key === \u0026#39;w\u0026#39; ? SPEED : -SPEED)); } else { box.target = undefined; } } fbo1.box(box.size); fbo1.pop(); } ); } function scene2() { boxes.forEach(box =\u0026gt; { fbo2.push(); fbo2.fill(boxes[box_key] === box ? color(\u0026#39;red\u0026#39;) : box.color); fbo2.translate(box.position); fbo2.box(box.size); fbo2.pop(); } ); } function keyPressed() { // press [0..9] keys to pick a box and other keys // to unpick, excepting \u0026#39;w\u0026#39; and \u0026#39;z\u0026#39; which are used // to move the box away or closer to eye. if (key !== \u0026#39;w\u0026#39; \u0026amp;\u0026amp; key !== \u0026#39;z\u0026#39;) { box_key = parseInt(key); } } "},{"id":8,"href":"/showcase/docs/Rendering/2.-Pixel-Density/","title":"2. Pixel Density","section":"Rendering","content":" The world and the pixel # Each pixel on the screen represents a quantity of space in the space of the world. The illusion of forced perspective allows to keep the relation that exists between the world and the pixel, that is to say, the amount of world that can be represented in the pixel in the space of the screen, this relation is going to be called density and is used by the function pixelRatio in the code previously seen.\nMove objects away # When the object is moved away from the screen on which it is being displayed, because of the angle of the frustum, the object reduces its size on the screen, however, in the world it still retains its size. This means that each pixel will contain more information about the world, even when the object is the same.\nInstructions The following figure shows a map being represented in a grid which represent the pixels on a screen\nMove the slider above to move the map further or closer to the screen Move the slider below to rotate our perception on the world (Hint: the map and the grid will rotate as well) Change Object\u0026rsquo;s Size # What would happen now if we changed the wor object\u0026rsquo;s size. The answer is that, in the same way that happened in when we changed the distance, the object would appear bigger or smaller on the screen.\nInstructions The following figure shows a map being represented in a grid which represent the pixels on a screen\nMove the slider above to move the map further or closer to the screen Move the slider in the middle to change the map\u0026rsquo;s size Move the slider below to rotate our perception on the world (Hint: the map and the grid will rotate as well) What would happen then if we increase the size while the scene is zoomed out? If we increase the size of the object at the same rate at which the field of view angle increases, what would happen is that the scene generated on the screen would not change, even though the size of the object does change.\nWhat does change is the pixel density, because even though the object is larger in the world, it is the same size on the screen, i.e., in this case the amount stored in each pixel of the world increases.\n"},{"id":9,"href":"/showcase/docs/Rendering/3.-Aplications/","title":"3. Aplications","section":"Rendering","content":" Concept 1 # In the present example an intuitive demonstration of forced perception is made. Two spheres are used which, in perspective, look the same size. However, one of them can be resized in such a way that it changes size in the world, but the camera perceives them as the same size. This happens because of the distance in relation to the camera.\nInstructions\nSelect any of the spheres with a number (0 to 2) Press w or s to zoom in or zoom out on the sphere On the left screen you will see that the spheres do not change size. On the right screen you will see the difference in size and distance between them. Concept 2 # In this example we used perspective and size to give 2 different images depending on the point of view, at the beginning the camera is in such a way that it seems that the chicken eats the sphere, but if we change the perspective of camera 1 (left) now we can see the bird inside the sphere, in the second camera (the right side) we can see how nothing has changed but moving the sphere the new perspective becomes real and now the chicken is enclosed in the sphere.\nThis case, like so many others (for example grabbing the moon with the hand taking into account that because of its remoteness it is perceived as small), allowed to move the object without losing the size we see (we can make it actually fit with what we are looking for), this is the basis of the superliminal game.\nInstructions\nWith the mouse, move the camera until the bird appears to be inside the camera You can zoom in or zoom out with the mouse scroll wheel Select the sphere with the number key 0 Press w until on the right screen you see that the sphere catches the bird You can now move the left view back to see the trapped bird Concept 3 # The following is a visual effect achieved by changing the size of a sphere with respect to the plane shown. It shows how the grid fits into the background plane to generate the desired visual effect.\nInstructions\nSelect the sphere with the number key 0 Press w until the sphere disappears from view across the plane, and press an instant more Rotate the camera with the mouse and you will see the sphere against the background which will generate an optical effect. Pixel Ratio: # it is a function that takes the world and the eye ( the observer\u0026rsquo;s point) and returns the units of a world in a location, namely it gives a quantity of world in a pixel referencia : https://github.com/VisualComputing/p5.treegl/blob/main/p5.treegl.js\nResults # By means of the present examples it can be seen how changes in size at the same rate as changes in distance result in no apparent change in the camera and screen. However, when implementing the programs the following problems were encountered.\nThe size change is still noticeable in the space of the camera and screen, especially when zoomed in or when the key is pressed interruptedly.\nWhen the object is at very close distances to the camera the object changes its perceived size. Also, when the object crosses the camera, the view of the object is lost and sometimes this can cause problems to the scene, sometimes because the object increases its size disproportionately.\n"},{"id":10,"href":"/showcase/docs/Rendering/4.-Conslusions/","title":"4. Conslusions","section":"Rendering","content":" Conclusions # The Superliminal game is made possible by elements of rendering that occur in computer graphics. It is through the process of the rendering pipeline that it is possible to achieve the effect of forced perception on the players. Even though an object changes in the world, because of its location and size in the world, this change is neither perceivable in the camera nor projectable on the screen.\nHowever, the present work does not consider other attributes of rendering such as shadows and light, which affect the result of the projection of objects according to their depth. If this were the case, the shadows and light of a distant object will be perceived differently by the camera, even if its size is the same in the projection.\nThe triangle proportionality theorem is indispensable for the forced perception effect. It allows to calculate the new coordinates in x and y from the change occurring in z. This happens because in the world the object must keep the same angle with respect to the visual field so that its projection on the screen does not change.\nFinally, through this work it can be noted that pixels are a representation model of the world, i.e., they contain information about the world, such as color. However, sometimes the same pixel contains more information about the world, due to the remoteness of the objects.\nFuture work: # This work puts a starting point for thinking about new visual illusions from the effect of forced perception, which deceives human perception about what they are actually seeing through a screen. One can think of a new era of games in which perception plays an important role and brings an innovative effect, as well as in other applications of rendering such as animation and may also be the film industry.\nAlso, we can think of more intuitive ways of actions to change the size of objects in the world, making them different from pressing the keys w and z, so that the player understands in a better way what he is doing when he changes the size of objects in the world.\nOn the other hand, it is important to understand more clearly the concepts of human vision, in order to understand how attributes such as depth affect the forced perception of objects.\nFinally, all these transformations are done while both the camera and the object are still. A future application would be to realize the illusion of forced perception when there is motion in either of these two spaces.\nReferences # https://blog.playstation.com/2020/06/30/breaking-down-the-tech-behind-superliminals-mind-bending-illusions/\nhttps://findnerd.com/list/view/Computer-Graphics-Different-Spaces/6982/\nhttps://tfetimes.com/wp-content/uploads/2015/04/F.Dunn-I.Parberry-3D-Math-Primer-for-Graphics-and-Game-Development.pdf\nhttps://github.com/VisualComputing/p5.treegl/tree/main/examples/subliminal\n"},{"id":11,"href":"/showcase/docs/Rendering/","title":"Rendering","section":"Docs","content":" Introduction # One of the problems facing computer graphics is how to represent a 3D shape or model on a 2D monitor or screen. This is relevant, because the monitors on which the images are displayed are not continuous spaces like the real world, they are, instead, represented by a grid of pixels, which are a discrete units that can only take one color value.\nThe computer then saves the model of the representation of the scene, which will be displayed on the screen, this process is called rendering, and is a process carried out in animation, video games, 3D modeling, among others.\nIn this paper we will present how the rendering process, known as the graphic pipeline allows a forced perspective of which the game Superliminal is based.\nRendering # When rendering a scene, the main objective is to choose a model or object which will be rendered (it will have its own position, orientation and zoom) and a screen represented in two dimensions, to which the objecy will be translated. The rendering is done by changing different coordinate spaces. These coordinate spaces are:\nModel Space # This is the x,y,z coordinate space under which the object geometry is defined, usually the object is defined by primitives in this space.\nWorld Space # The world is the space under which the whole scene is defined, it has its origin in the center of the scene. It is possible to go from the model space to the world space by means of a model transformation.\nEye Space or Camera # It is the coordinated space with origin in the center of the projection. The camera is the point of view from which the world is seen, it forms a frustum formed from four rays. Every object falling within the spectrum of the four rays will be perceived by the camera, as points in the world are reflected as rays back to the camera.\nZoom # One of the characteristics of the eye space is the zoom. This originates from the fact that the frustum forms angles with the x and y coordinate axes. The larger these angles are, the more of the world space will be percieved in the projection and vice versa.\nThe zoom would allow the camera to increase the space it perceives by increasing its angles, as long as these maintain the proportionality of the image.\nScreen Space # The screen space is a projection from the camera to a 2D space. This is used to project the camera space to a representation on monitors as it is known today on computers.\nGraphic Pipieline # This is a process that describes the steps of rendering an object to its 2D form or, rather, from object space to screen space.\nSuperliminal # Superliminal is a video game from the first person perspective. The plot of the game consists of a player who finds himself trapped in a surreal place called the dream space and must find ways to escape from that place. However, a feature of the dream space mentioned above is that the player can change the size of objects using forced perspective.\nForced perspective # Forced perspective is a phenomenon in which the observer is deceived about the size of an object, i.e. it may appear larger or smaller than it actually is. This occurs because the representation of the scene in the human eye, in this case the camera, perceives the object in a way that appears different from reality.\nIn the specific case of the game, the player has control of the forced perspective, in that he can increase and decrease the size of the objects in the world without changing the size under the perspective of the camera. This is achieved because before the screen space that is generated by the rendering process, the created scene does not change even though in the world the object is changing.\n"},{"id":12,"href":"/showcase/docs/Shaders/1.-Texturing/","title":"1. Texturing","section":"Shaders","content":" Texturing # Las texturas en la computación gráfica son elementos los cuales añaden detalles a una imagen. Se pueden pensar las texturas como una imagen que cubre el objeto. Así como cuando se cubre un regalo con una envoltura, se debe asignar cada punto del objeto a un punto de la envoltura.\nEs por esta razón que, cuando se trata de texturas, es necesario dividir la imagen en partes más pequeñas para que concuerden con cada parte de la imagen de la textura.\nNo obstante, para aplicar la textura sobre una figura es necesario utilizar las coordenadas de textura o las coordenadas UV o ST. Estas son un espacio en 2 dimensiones formado por los vértices de la figura:\nDe esta manera, para aplicar una textura sobre una figura, se hace un mapeo entre las coordenadas de la textura hacia las coordenadas de la figura. Como este mapero este mapeo se hace pixel por pixel, los shaders son utilizados, especialmente los fragment shaders, ya que estos posibilitan dividir la figura en fragmentos pequeños, conocidos como los pixeles para hacer el mapeo de estas coordenadas, todo esto de manera paralela.\n"},{"id":13,"href":"/showcase/docs/Shaders/1.-Texturing/Color-brightness/","title":"Color Brightness","section":"1. Texturing","content":" Color brightness tools # Ejercicio 2: Texture Sampling # El modelo de color HSV ( Hue, Saturation, Value. Por sus siglas en inglés) también es llamado HSB (Hue, Saturation, Brightness - Matiz) fue creado en 1978 por Alvy Ray Smith.\nEs una transformación no lineal de color RGB y se usa para progreciones de color, diferente al modelo HSL.\nEn la imagen podemos ver el cono del modelo HSV, podemos movernos a través del mismo con un vector de 3 dimensiones de forma:\n0º = RGB(1, 0, 0) 60º = RGB(1, 1, 0) 120º = RGB(0, 1, 0) 180º = RGB(0, 1, 1) 240º = RGB(0, 0, 1) 300º = RGB(1, 0, 1) 360º = 0º A continuación se muestra un ejemplo de esto:\nInstrucciones\nEn el selector, escoja el grado de rotación en el cono del modelo HSV Foto de Md. Noor Hossain: https://www.pexels.com/es-es/foto/vacaciones-gente-multitud-ceremonia-8443591/\nHSL (Hue, Saturation, Lightness. Por sus siglas en inglés), también llamado HSI (Hue, Saturation, Intensity. Por sus siglas en inglés), tiene dos vertices que representan el blanco y el negro, el ángulo se corresponde con el Hue, la distancia con la Saturation y la distancia al eje blanco-negro con el Lightness\ntransformaciones a HSV y HSL\nInstrucciones\nEn el selector, escoja la trasformación HSV, HSL o la imagen original Foto de Belle Co: https://www.pexels.com/es-es/foto/foto-de-una-tortuga-bajo-el-agua-847393/\n"},{"id":14,"href":"/showcase/docs/Shaders/1.-Texturing/Procedural-texturing/","title":"Procedural Texturing","section":"1. Texturing","content":" Procedural texturing # Ejercicio 3 # Las texturas procedimentales son aquellas que se realizan por medio de un una definición matemática. Estas utilizan variables del entorno como el tiempo, la posición del mouse, la posición de los vértices y pixeles entre otras para asignar una textura a una imagen.\nPara realizar una textura sobre un objeto es necesario realizar un mapero entra las coordenadas de la figura y el modelo de la figura, siendo este mapeo usualmente lineal. Sin embargo, si se utiliza otras funciones para mapear del espacio del modelo hacia la figura se obtienen distintos patrones sobre la figura.\nTambién se pueden utilizar funciones dicretas de manera que el cambio en la textura no sea continuo sino por pasos.\nDe esta manera se pueden obtener diversas figuras (líneas, curvas, círculos, entre otros) por medio de la apliación de funciones no lineales a las coordenadas de texturas.\nAplicación # Entonces, se pueden crear las texturas por medio de un fragment shader. Este recibe la información de pixel a pixel y, de acuerdo a sus coordenadas ST, se mapea un color sobre la figura de la siguiente manera:\nvec2 tile (vec2 _st, float _zoom) { _st *= _zoom; return fract(_st); } En esta sección del código se reciben las coordenas ST y un atributo llamado _zoom. En esta función, se modifican las coordenadas de cada pixel de manera que se devuelva su parte fraccional.\n\\[ f(x) = x - floor(x) \\] Esta función es luego usada para asignar el color de cada pixel sobre la figura:\nst = tile(st,u_zoom*0.5); gl_FragColor = vec4(vec3(step(st.x,st.y)),1.0); Este es el proceso mediante el cual se generan texturas procedimentales. Para crear nuevas texturas se pueden utilizar nuevas funciones matemáticas. A continuación, se muestran varios ejempos de texturas procedimentales sobre varias figuras.\nInstrucciones\nEn el primer selector, selecciona la figura sobre la cual aplicar la textura En el segundo slector, selecciona la textura a aplicar Mueve el mouse para cambiar el tamaño de la textura Haz cliz y mueve el mouse para cambiar de perspectiva de la cámara "},{"id":15,"href":"/showcase/docs/Shaders/1.-Texturing/UV-Visualization/","title":"Uv Visualization","section":"1. Texturing","content":" UV vizualization # Ejercicio: UV Vizualization # Mediante este ejercicio se puede demostrar la transformación de las coordenadas UV a un rectángulo. Utilizando las posiciones en las coordenadas de textura UV para mapear el color que toma el pixel en la imágen.\n"},{"id":16,"href":"/showcase/docs/Shaders/2.-Image-Processing/Image-Processing/","title":"Image Processing","section":"2. Image Processing","content":" Que es el Image Processing # El image Processing es el uso de computadores para procesar imagenes digitales mediante algun algoritmo, en este se permiten un aplio rango de algoritmos para ser aplicados sobre la entrada, ademas puede evitar problemas como el ruido y las distorciones.\nEn este caso se usan matrices de convolucion o mascaras, se les tiende a llamar matrices de convolucion por el proceso que tienen para empezar se crea una matriz de n*m \\[ \\begin{vmatrix} 0 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 1 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 0 \\end{vmatrix}\\ \\] en este caso de 3x3 esta matriz recorrera la matriz original de la imagen y genera una nueva imagen, en este caso al ser la matriz identidad la imagen generada es la misma, estas imagenes tienden a destacar atributos de la imagen por ejemplo \\[ \\begin{vmatrix} -1 \u0026amp; -1 \u0026amp; -1\\\\ -1 \u0026amp; 8 \u0026amp; -1\\\\-1 \u0026amp; -1 \u0026amp; -1 \\end{vmatrix}\\] esta matriz se usa para identificar los cambios de relieve de una imagen\nImage processing Tool # En esta herramienta se implementaron diferentes matrices de convolucion para poder procesar una imagen y se genero un foco para poder ver donde se aplica .\nInstrucciones\nEn el primer seleccionador, escoja el filtro a aplicar Escoja foco en la caja de selección si desea aplicar un foco en el filtro.Una vez se aplique el foco, el filtro se aplicará sobre la seccion de la imagen debajo del mouse y no en toda la imagen. Utilice el desilizador para amuentar o disminuir el tamaño del foco Video processing Tool # Gracias al mismo principio se puede usar en videos ya que estos son una serie de imagenes solo se debe aplicar la máscara a cada frame.\nInstrucciones\nEn el primer seleccionador, escoja el filtro a aplicar Escoja foco en la caja de selección si desea aplicar un foco en el filtro.Una vez se aplique el foco, el filtro se aplicará sobre la seccion de la imagen debajo del mouse y no en toda la imagen. Utilice el desilizador para amuentar o disminuir el tamaño del foco "},{"id":17,"href":"/showcase/docs/Shaders/2.-Image-Processing/magnifierTool/","title":"Magnifier Tool","section":"2. Image Processing","content":" Tool of magnifier # Esta apliación permite utilizar un fragment shader para amplificar el tamaño de la imagen y de un video alreder de un foco.\nInstrucciones\nEscoja video o imagenen la caja de selección Utilice el desilizador para amuentar o disminuir el tamaño del foco "},{"id":18,"href":"/showcase/docs/Shaders/3.-Extra-job/","title":"3. Extra Job","section":"Shaders","content":" Trabajo extra # En este trabajo se presentan dos distintas apliaciones de los shaders el bump mapping y el shadow mapping.\nBump mapping # Esta es una técnica de textura que permite generar relieves sobre una figura, esto se hace modificando las normales de a superficie.\nLos bump maps utilizan la iluminación para generar estos tipos de texturas. Por ejemplo, los colores más claros parececen resaltar de las superficies de la textura, mientras que los colores más oscuros parecen estar hundidos dentro de la superficie. Es este el principio utilizado para generar relieves sin necesariamente modificar la geometría de la figura, solo su color, como se muestra a continuación.\nShadow mapping # El shadow mapping consiste en la renderización desde el punto de vista de la fuente de luz. En esta se reliza un trazo desde la fuente de iluminación hasta los puntos de los objetos a los que esta alacanza a llegar, teniendo en cuenta sus ángulos.\nDe tal manera, como hay lugares a los que la luz no llega, o llega con un menor ángulo, esto se refeja en el hecho de que hay puntos de la imagen que quedan más oscuros.\nApliación # El presente es un sistema solar que utiliza ambos fundamentos.\nEl bump mapping se representa mediante el sol: este se hace por medio de agregar ruido al color pixel a pixel en el fragment shade. Además se apoya en las modificación de posiciones de cada pixel para este fin. El shadow mapping se representa por medio de los planetas, estas son esferas cuyos pixeles adquieren su color con respecto a la distancia que estos tienen de la fuente de iluminación que es el sol. "},{"id":19,"href":"/showcase/docs/Shaders/4.-Conclusions/","title":"4. Conclusions","section":"Shaders","content":" Conclusiones # Los shaders permiten que la computación gráfica sea más rápida y efectiva. Esto se puede ver reflejado en el procesamiento de imágenes, en el cual, para aplicar un filtro, es necesario hacer transformaciones pixel a pixel. En algunas imágenes estos son millones de tareas por realizar. Por tal motivo, paralelizar estos procesos ha permitido que la apliación de filtros sea más rápida, previniendo demoras que puedan entorpecer la experiencia de usuario.\nAsimismo, se pemite trabajar con texturas, demostrando que, sin necesidad de tener ninguna imagen preliminar, únicamente basandose en la geometría de los objetos y de cada uno de sus pixeles (como su posisción, su luminosidad, su color) y de las funciones y propiedades matemáticas se pueden generar diversos patrones sobre los objetos.\nSin embargo, a la hora de mapear una textura sobre una figura, es posible que en las las aristas estas texturas se muestren de manera distinta para cada lado de la arista, mostrando una discontinuidad entre cada uno de los espacios generados por los vértices. Esto ocasiona que las texturas no se vean de manera natural en esos puntos.\nTrabajo futuro # Basandose en las texturas procedimentales ya conocidas y en funciones mátemáticas que no se hayan usado para este fin, se puede trabajar en la búsqueda de nuevas texturas procedimentales. Además, las texturas procedimentales realizadas en este taller fueron todas hechas en blanco y negro, por lo que, en un futuro, se podría trabajar con escalas de grises y con todos los colores RGB.\nEn cuanto a las discontinudades que se generan en las texturas entre las aristas de las figuras, se puede utilizar los visto en la Banda de Mach para que el contraste entre estas aristas sean menos visible ante el ojo humano.\nSe puede pensar, además, en desarrollar una implementación de bump mapping desde un vertex shader, de manera que los relieves se realicen desde los vértices.\nReferencias # https://www.scratchapixel.com/lessons/3d-basic-rendering/introduction-to-shading/procedural-texturing https://thebookofshaders.com/ https://en.wikipedia.org/wiki/Bump_mapping#:~:text=Bump%20mapping%20is%20a%20texture,perturbed%20normal%20during%20lighting%20calculations. https://learnopengl.com/Advanced-Lighting/Shadows/Shadow-Mapping "},{"id":20,"href":"/showcase/docs/Shaders/","title":"Shaders","section":"Docs","content":" Shaders # La renderización es un proceso mediante el cual se transformauna figura o modelo a una pantalla de 2D, haciendo diversas transformaciones entre espacios coordenados. Sin embargo, este proceso ha representado una gran complejidad en la computación gráfica, debido que los monitores que se presentan alcanzan a tener millones de pixeles que procesar.\nTomada de: The book of shaders https://thebookofshaders.com/01/\nEsto representa un problema, teniendo en cuenta que tradicionalmente los procesadores realizan estas tareas de manera secuencial, es decir que por cada frame mostrado en un monitor, se deben realizar millones de operaciones, sin tener en cuenta los otros procesos que lleva a cabo el computador fuera de estas tareas. Asimismo, esto también ignora que normalmente una pantalla muestra de 30 a 60 frames cada segundo. A pesar de la velocidad de los procesadores actuales, un solo procesador en insuficiente para realizar la cantidad de procesos mencionada.\nTomada de: The book of shaders https://thebookofshaders.com/01/\nComo solución al problema mencionado, se presentan los shaders. Estos son programas que se corren en una unidad de procesamiento gráfico (GPU), las cuales tienen dentro de ellas múltiples núcleos de procesamiento. Por lo tanto, esto significa que las tareas van a ser ahora paralelizables, es decir, que se realicen de manera simultánea. Con el uso de shaders es posible reducir el tiempo de renderización al realizar operaciones sobre varios pixeles al mismo tiempo.\nTomada de: The book of shaders https://thebookofshaders.com/01/\nLos shaders son un concepto que por completo cambiaron la computación gráfica, ya que estos permiten el procesamento de características de distintos objetos de manera paralalela por medio de las unidades de procesamiento, permitiendo su realización más rápida y dettallada, permitiendo añadir a las imágenes características como la iluminación, textura y sombreado.Es por esta razón que es utilizada en animación, video juegos, creaciónd de modelos 3D, entre otras.\nVertex Shaders # Estos son programas que obtienen imformación de los vertices de la figura a renderizar, este se encarga del tratamiento de cada vértice individualmente. Las variables de cada vértice que puede tratar son:\nLa posición en x, y, z El color La textura La ilumincación Como se muestra en la siguiente imagen, los puntos que se muestran en azul son los vértices, los cuales serán tratados por el vertex shader cuando se renderice la figura.\nOtra función del vertex shader es de interpolar atributos al fragment shader.\nFragment shaders # Estos son programas que obtienen imformación de cada pixel de la figura a renderizar, este se encarga del tratamiento de cada uno de los pixeles individualmente, recibiendo atributos del vertex shader. Las variables de cada pixel que se puede tratar son:\nLa posición en x, y, z El color La textura La ilumincación Ahora, son todos los elementos de la figura, no solo los vértices, los que se trataran en el fragment shader.\nNormalmente, los vertex shaders son utilizados para la modificación de la geometría de la figura y los framgment shader para modificar atributos como color, textura e iluminación.\nA continuación, se mostrarán los ejercicios realizados en clase de las apliaciones de los shaders.\n"}]